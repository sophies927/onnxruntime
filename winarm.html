<!DOCTYPE html>
<html lang="en">

<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156955408-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'UA-156955408-1');
    </script>

    <script type="text/javascript" src="https://js.monitor.azure.com/scripts/c/ms.analytics-web-3.min.js">
    </script>

    <script type="text/javascript">
        const analytics = new oneDS.ApplicationInsights();
        var config = {
            instrumentationKey: "360b0e675e0044398fd28c8bdf711b8e-1fe5434d-ee99-4837-99cc-a3a16462d82d-7262",
            channelConfiguration: { // Post channel configuration
                eventsLimitInMem: 50
            },
            propertyConfiguration: { // Properties Plugin configuration 
                env: "PPE" // Environment can be set to PPE or PROD as needed. 
            },
            webAnalyticsConfiguration: { // Web Analytics Plugin configuration
                //urlCollectQuery:true, 
                autoCapture: {
                    scroll: true,
                    pageView: true,
                    onLoad: true,
                    onUnload: true,
                    click: true,
                    resize: true,
                    jsError: true
                }
            }
        };
        //Initialize SDK
        analytics.initialize(config, []);
    </script>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>ONNX Runtime | Windows Dev Kit 2023</title>
    <link rel="icon" href="./images/ONNXRuntime-Favicon.png" type="image/gif" sizes="16x16">
    <link rel="stylesheet" href="css/fonts.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/4.6.2/css/bootstrap.min.css"
        integrity="sha512-rt/SrQ4UNIaGfDyEXZtNcyWvQeOq0QLygHluFQcSjaGB04IxWhal71tKuzP6K8eYXYB6vJV4pHkXcmFGGQ1/0w=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"
        integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="css/custom.css">
    <link rel="stylesheet" href="css/responsive.css">
</head>

<body>
    <a class="skip-main" href="#skipMain">Skip to main content</a>
    <div class="main-wrapper">
        <div class="top-banner-bg">

            <!-- Partial header.html Start-->
            <div w3-include-html="header.html"></div>
            <!-- Partial header.html End-->

            <div role="main" id="skipMain" tabindex="-1">

                <div class="container px-md-4 px-lg-5 pt-5 mx-auto text-center">
                    <h1 class="pt-3 pb-3 pt-md-5 pb-lg-3 px-md-4 px-lg-5 mt-5 mb-0">ONNX Runtime + Windows Dev Kit 2023
                        = NPU powered AI</h1>
                </div>

                <div class="outer-container mx-auto">
                    <section class="pt-4 blue-title-columns">
                        <div class="container-fluid">
                            <div class="row">
                                <div class="col-12 col-md-12 pr-5">
                                    <h2 class="blue-text">Delivering NPU powered AI capabilities in your apps</h2>
                                    <p>Windows Dev Kit 2023, aka Project Volterra, enables developers to build apps that
                                        unlock the power of the NPU hardware to accelerate AI/ML workloads delivering
                                        AI-enhanced features & experiences without compromising app performance.</p>
                                    <p>You can get started now and access the power of the NPU through the open source
                                        and cross-platform ONNX Runtime inference engine making it easy to run AI/ML
                                        models from popular machine learning frameworks like PyTorch and TensorFlow.</p>
                                </div>
                            </div>
                        </div>
                    </section>
                    <hr>
                    <section class="pb-4 pt-4 blue-title-columns">
                        <div class="container-fluid">
                            <div class="row pb-4 pb-md-5">

                                <div class="col-12 col-md-6 mb-4 mb-md-0 text-center pr-10">
                                    <img src="./images/windowsdevkit2023.png" class="img-fluid">
                                </div>
                                <div class="col-12 col-md-6 pr-5">
                                    <h2 class="blue-text">Get started on your Windows Dev Kit 2023 today</h2>
                                    <p>Follow these steps to setup your device to use ONNX Runtime (ORT) with the built
                                        in NPU:
                                    <ol>
                                        <li><a href="https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk/windows-on-snapdragon"
                                                target="_blank">Request access</a> to the Neural Processing SDK for
                                            Windows on Snapdragon. Qualcomm may reach out to you via email with further
                                            registration instructions for approval. </li>
                                        <li>Once approved, you will receive an email with links to download SNPE.
                                            <ol type="a">
                                                <li>Select the SNPE link which takes you to a Qualcomm login and
                                                    download page. </li>
                                                <li>Select the <i>Snapdragon_NPE_SDK.WIN.1.0 Installer</i> link,
                                                    download and install.</li>
                                            </ol>
                                        <li><a href="https://www.nuget.org/packages/Microsoft.ML.OnnxRuntime.Snpe"
                                                target="_blank">Download</a> and install the ONNX Runtime with SNPE
                                            package</li>
                                        <li>Start using the ONNX Runtime API in your application.</li>
                                    </ol>
                                    </p><br />
                                    <h2 class="blue-text">Optimizing models for the NPU</h2>
                                    <p><a href="https://onnx.ai" target="_blank">ONNX</a> is a standard format for
                                        representing ML models authored in frameworks like PyTorch, TensorFlow, and
                                        others. ONNX Runtime can run any ONNX model, however to make use of the NPU, you
                                        currently need to use the following steps:
                                    <ul>
                                        <li>Run the tools provided in the SNPE SDK on your model to generate a binary
                                            file.</li>
                                        <li>Include the contents of the binary file as a node in the ONNX graph.</li>
                                    </ul>
                                    <p>See our <a
                                            href="https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_sharp/Snpe_EP/vgg16_image_classification"
                                            target="_blank">C# tutorial<a> for an example of how this is done.</p>
                                    <p>Many models can be optimized for the NPU using this process. Even if a model
                                        cannot be optimized for NPU by the SNPE SDK, it can still be run by ONNX Runtime
                                        on the CPU.</p>
                                    <br />
                                    <h2 class="blue-text">Tutorials</h2>
                                    <ul>
                                        <li><a href="https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_sharp/Snpe_EP/vgg16_image_classification"
                                                target="_blank">C# Image classification with VGG16 using ONNX Runtime
                                                with SNPE<a></li>
                                        <li><a href="https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_cxx/Snpe_EP"
                                                target="_blank">C++ image classification with Inception v3 using ONNX
                                                Runtime with SNPE<a></li>
                                    </ul>
                                    <br />
                                    <h2 class="blue-text">Getting help</h2>
                                    <p>For help with ONNX Runtime, you can <a
                                            href="https://github.com/microsoft/onnxruntime/discussions"
                                            target="_blank">start a discussion</a> on GitHub or <a
                                            href="https://github.com/microsoft/onnxruntime/issues" target="_blank">file
                                            an issue</a>.
                                    <p>
                                </div>
                            </div>
                        </div>
                    </section>


                </div>
            </div>
        </div>
    </div>
    <!-- Partial footer.html Start-->
    <div w3-include-html="footer.html"></div>
    <!-- Partial footer.html End-->

    <a id="back-to-top" href="JavaScript:void(0);" class="btn btn-lg back-to-top" role="button"
        aria-label="Back to top"><span class="fa fa-angle-up"></span></a>

    <script src="./js/w3.js"></script>
    <script>w3.includeHTML();</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js"
        integrity="sha512-3gJwYpMe3QewGELv8k/BX9vcqhryRdzRMxVfq6ngyWXwo03GFEzjsUm8Q7RZcHPHksttq7/GFoxjCVUjkjvPdw=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/4.6.2/js/bootstrap.min.js"
        integrity="sha512-7rusk8kGPFynZWu26OKbTeI+QPoYchtxsmPeBqkHIEXJxeun4yJ4ISYe7C6sz9wdxeE1Gk3VxsIWgCZTc+vX3g=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <script src="./js/custom.js"></script>

</body>

</html>f